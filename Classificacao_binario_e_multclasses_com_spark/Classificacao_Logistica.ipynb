{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64e677e",
   "metadata": {},
   "source": [
    "#  Classificação Logística com Spark usando PySpark\n",
    "\n",
    "\n",
    "\n",
    "## Inicializando o PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecfeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Faz a Interafce entre o Spark e o Jupyter Notebook\n",
    "findspark.init()\n",
    "\n",
    "# Inicializando uma Sessão no Spark\n",
    "spark = SparkSession.builder.appName(\"Classificação Logística\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d23be",
   "metadata": {},
   "source": [
    "### Hiper Parâmetros\n",
    "\n",
    "> + **link**: define a função de link. Valores possíveis: identity, log, inverse, logit, probit, cloglog e sqrt\n",
    "> + **maxIter**: número máximo de interações no treinamento do modelo. (padrão 100) \n",
    "> + **regParam**: valor de regularização. (padrão 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcc5fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Aplicando  Classificação Logística ao DataSet Churn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f45af4",
   "metadata": {},
   "source": [
    "### Carregando o Cunjunto de Dados Carros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a673a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Registros do Dataset: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
      "|CreditScore|Geography|Gender|Age|Tenure| Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
      "|        619|   France|Female| 42|     2|       0|            1|        1|             1|       10134888|     1|\n",
      "|        608|    Spain|Female| 41|     1| 8380786|            1|        0|             1|       11254258|     0|\n",
      "|        502|   France|Female| 42|     8| 1596608|            3|        1|             0|       11393157|     1|\n",
      "|        699|   France|Female| 39|     1|       0|            2|        0|             0|        9382663|     0|\n",
      "|        850|    Spain|Female| 43|     2|12551082|            1|        1|             1|         790841|     0|\n",
      "+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churn = spark.read.csv(\"../Material_do_Curso/Churn.csv\",\n",
    "                             header=True, inferSchema=True, sep=\";\")\n",
    "print(f\"Quantidade de Registros do Dataset: {churn.count()}\")\n",
    "churn.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a006167",
   "metadata": {},
   "source": [
    "### Escolha das Variáveis Independentes e Dependente para treinamento do Modelo\n",
    "\n",
    "Para esse exemplo, as variáveis dependentes serão os atritutos  **Consumo | Cilindros | Cilindradas |** e a variável Dependente será o Atributo **| HP |**\n",
    "\n",
    "ou seja, vamos usar as variáveis independentes para prever a dependente, para isso, vamos treinar o modelo com os dados do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3925284",
   "metadata": {},
   "source": [
    "### Importação do Módulo do PySpark Para o Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bfe6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba2d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+----------+\n",
      "|independente                                                  |dependente|\n",
      "+--------------------------------------------------------------+----------+\n",
      "|[619.0,1.0,0.0,0.0,42.0,2.0,0.0,1.0,1.0,1.0,1.0134888E7]      |1.0       |\n",
      "|[608.0,0.0,0.0,0.0,41.0,1.0,8380786.0,1.0,0.0,1.0,1.1254258E7]|0.0       |\n",
      "|[502.0,1.0,0.0,0.0,42.0,8.0,1596608.0,3.0,1.0,0.0,1.1393157E7]|1.0       |\n",
      "|(11,[0,1,4,5,7,10],[699.0,1.0,39.0,1.0,2.0,9382663.0])        |0.0       |\n",
      "|[850.0,0.0,0.0,0.0,43.0,2.0,1.2551082E7,1.0,1.0,1.0,790841.0] |0.0       |\n",
      "+--------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Colocando os dados no Formato de dataframe do Spark usando o RFormula\n",
    "rformula = RFormula(formula=\"Exited ~ .\",\n",
    "                    featuresCol=\"independente\",\n",
    "                    labelCol=\"dependente\")\n",
    "churn_rf = rformula.fit(churn).transform(churn)\n",
    "churn_rf.select(\"independente\", \"dependente\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a115d5",
   "metadata": {},
   "source": [
    "### Separando os Dados entre conjunto de Treino e Teste\n",
    "\n",
    "O Conjunto de dados serão separados entre treino e teste, sendo que **80%** será para treinar o modelo e **20%** para testar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ad20c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Dados de Treino: 8008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Dados de Teste: 1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "churn_train, churn_test = churn_rf.randomSplit([0.8, 0.2])\n",
    "print(f\"Quantidade de Dados de Treino: {churn_train.count()}\")\n",
    "print(f\"Quantidade de Dados de Teste: {churn_test.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b5bf6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Importação do Módulo do PySpark Para Criação do Modelo de Classificação Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b15be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01232e6e",
   "metadata": {},
   "source": [
    "#### Instanciando Objeto e criando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d8cdae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 23:46:14 WARN BlockManager: Asked to remove block broadcast_44, which does not exist\n"
     ]
    }
   ],
   "source": [
    "# Instanciando o objeto LinearRegression\n",
    "obj_lr = LogisticRegression(featuresCol=\"independente\",\n",
    "                            labelCol=\"dependente\",\n",
    "                            maxIter=1000, standardization=True)\n",
    "\n",
    "# Criando o Modelo\n",
    "model_lr = obj_lr.fit(churn_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae146dcd",
   "metadata": {},
   "source": [
    "### Parâmetros do Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8e1ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.8124375624375625\n",
      "Presisão: 0.782087319536303\n",
      "recall: 0.8124375624375624\n",
      "Área Sobre a curva: 0.7655822818870617\n"
     ]
    }
   ],
   "source": [
    "resumo = model_lr.summary\n",
    "acuracia = resumo.accuracy\n",
    "precisao = resumo.weightedPrecision\n",
    "recall = resumo.weightedRecall\n",
    "auc = resumo.areaUnderROC\n",
    "\n",
    "print(f\"Acurácia: {acuracia}\")\n",
    "print(f\"Presisão: {precisao}\")\n",
    "print(f\"recall: {recall}\")\n",
    "print(f\"Área Sobre a curva: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b952849",
   "metadata": {},
   "source": [
    "#### Realizando Presição com o Modelo Criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53e70eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------------------------------------+--------------------------------------------+\n",
      "|dependente|prediction|probability                             |rawPrediction                               |\n",
      "+----------+----------+----------------------------------------+--------------------------------------------+\n",
      "|1.0       |0.0       |[0.8437003172962992,0.15629968270370076]|[1.6860221498699997,-1.6860221498699997]    |\n",
      "|1.0       |0.0       |[0.7521793673290733,0.2478206326709267] |[1.110269586762303,-1.110269586762303]      |\n",
      "|1.0       |1.0       |[0.49778895042881327,0.5022110495711867]|[-0.008844255934740364,0.008844255934740364]|\n",
      "|1.0       |0.0       |[0.847871693250834,0.152128306749166]   |[1.718005031091091,-1.718005031091091]      |\n",
      "|1.0       |0.0       |[0.7505473314354185,0.24945266856458148]|[1.1015335235778099,-1.1015335235778099]    |\n",
      "+----------+----------+----------------------------------------+--------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previsao_test = model_lr.transform(churn_test)\n",
    "previsao_test.select(\"dependente\", \"prediction\", \"probability\", \"rawPrediction\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492374a",
   "metadata": {},
   "source": [
    "### Avaliando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6ed9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7443c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderRoc: 0.7734546915379094\n"
     ]
    }
   ],
   "source": [
    "avaliar = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n",
    "                                        labelCol=\"dependente\",\n",
    "                                        metricName=\"areaUnderROC\")\n",
    "\n",
    "areaUnderRoc = avaliar.evaluate(previsao_test)\n",
    "\n",
    "print(f\"areaUnderRoc: {areaUnderRoc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ed76a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
