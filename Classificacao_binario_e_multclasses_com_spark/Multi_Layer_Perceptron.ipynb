{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64e677e",
   "metadata": {},
   "source": [
    "<center> <h1 style=\"background-color:seagreen; color:white\" >Classificação com Rede Neural Artificial (Multi Layer Perceptron) com Spark usando PySpark</h1> \n",
    "\n",
    "\n",
    "<center> <h2 style=\"background-color:DarkKhaki; color:white\" >Inicializando o PySpark</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecfeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Faz a Interafce entre o Spark e o Jupyter Notebook\n",
    "findspark.init()\n",
    "\n",
    "# Inicializando uma Sessão no Spark\n",
    "spark = SparkSession.builder.appName(\"NaiveBayes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d23be",
   "metadata": {},
   "source": [
    "### Hiper Parâmetros\n",
    "\n",
    "> + **layers**: camadas\n",
    "> + **seed**: semente\n",
    "> + **stepSize**: o quanto os pesos serão atualizados em cada iteração (learning\n",
    "rate). (padrão: 0.03)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dcc5fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center> <h2 style=\"background-color:DarkKhaki; color:white\" >Aplicando Multi Layer Perceptron no DataSet Íris</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f45af4",
   "metadata": {},
   "source": [
    "### Carregando o Cunjunto de Dados Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a673a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Registros do Dataset: 150\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|class      |\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "|5.1        |3.5       |1.4        |0.2       |Iris-setosa|\n",
      "|4.9        |3.0       |1.4        |0.2       |Iris-setosa|\n",
      "|4.7        |3.2       |1.3        |0.2       |Iris-setosa|\n",
      "|4.6        |3.1       |1.5        |0.2       |Iris-setosa|\n",
      "|5.0        |3.6       |1.4        |0.2       |Iris-setosa|\n",
      "+-----------+----------+-----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = spark.read.csv(\"../Material_do_Curso/iris.csv\",\n",
    "                             header=True, inferSchema=True, sep=\",\")\n",
    "print(f\"Quantidade de Registros do Dataset: {iris.count()}\")\n",
    "iris.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c754d6",
   "metadata": {},
   "source": [
    "### Criando uma vetorização com VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b49dd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3cd074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----------+-----------------+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|      class|     independente|\n",
      "+-----------+----------+-----------+----------+-----------+-----------------+\n",
      "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|        4.9|       3.0|        1.4|       0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
      "|        4.6|       3.1|        1.5|       0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
      "|        5.0|       3.6|        1.4|       0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
      "+-----------+----------+-----------+----------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "asb = VectorAssembler(inputCols=[\"sepallength\", \"sepalwidth\",\n",
    "                                 \"petallength\", \"petalwidth\"],\n",
    "                      outputCol=\"independente\")\n",
    "\n",
    "iris_asb = asb.transform(iris)\n",
    "iris_asb.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad38be6",
   "metadata": {},
   "source": [
    "### Transformando o rótulo **class** com o StringIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f5a30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2e11bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----------+-----------------+----------+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|      class|     independente|dependente|\n",
      "+-----------+----------+-----------+----------+-----------+-----------------+----------+\n",
      "|        5.1|       3.5|        1.4|       0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|       0.0|\n",
      "|        4.9|       3.0|        1.4|       0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|       0.0|\n",
      "|        4.7|       3.2|        1.3|       0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|       0.0|\n",
      "|        4.6|       3.1|        1.5|       0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|       0.0|\n",
      "|        5.0|       3.6|        1.4|       0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|       0.0|\n",
      "+-----------+----------+-----------+----------+-----------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ind = StringIndexer(inputCol=\"class\", outputCol=\"dependente\")\n",
    "iris_asb = ind.fit(iris_asb).transform(iris_asb)\n",
    "iris_asb.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a006167",
   "metadata": {},
   "source": [
    "### Escolha das variáveis independentes e dependente para treinamento do modelo dados do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3925284",
   "metadata": {},
   "source": [
    "### Importação do Módulo do PySpark Para o Pré-Processamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a115d5",
   "metadata": {},
   "source": [
    "### Separando os Dados entre conjunto de Treino e Teste\n",
    "\n",
    "O Conjunto de dados serão separados entre treino e teste, sendo que **80%** será para treinar o modelo e **20%** para testar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ad20c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Dados de Treino: 99\n",
      "Quantidade de Dados de Teste: 51\n"
     ]
    }
   ],
   "source": [
    "iris_train, iris_test = iris_asb.randomSplit([0.7, 0.3])\n",
    "print(f\"Quantidade de Dados de Treino: {iris_train.count()}\")\n",
    "print(f\"Quantidade de Dados de Teste: {iris_test.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b5bf6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center> <h1 style=\"background-color:DarkKhaki; color:white\" >Importação do Módulo do PySpark Para Criação do Modelo de Rede Neural Artificial </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81b15be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01232e6e",
   "metadata": {},
   "source": [
    "#### Instanciando Objeto e criando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96d8cdae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Instanciando o objeto LinearRegression\n",
    "obj_mlp = MultilayerPerceptronClassifier(maxIter=10, layers=[4, 5, 4, 3],\n",
    "                                        featuresCol=\"independente\", labelCol=\"dependente\")\n",
    "\n",
    "# Criando o Modelo\n",
    "model_mlp = obj_mlp.fit(iris_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b952849",
   "metadata": {},
   "source": [
    "#### Realizando Presição com o Modelo Criado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e70eb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|dependente|prediction|\n",
      "+----------+----------+\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "previsao_test = model_mlp.transform(iris_test)\n",
    "previsao_test.select(\"dependente\", \"prediction\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492374a",
   "metadata": {},
   "source": [
    "### Avaliando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ed9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7443c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avaliar = MulticlassClassificationEvaluator(labelCol=\"dependente\",\n",
    "                                            predictionCol=\"prediction\",\n",
    "                                            metricName=\"accuracy\")\n",
    "\n",
    "acuracia = avaliar.evaluate(previsao_test)\n",
    "\n",
    "print(f\"Acurácia: {acuracia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf24c62",
   "metadata": {},
   "source": [
    "### Pegando os valores dos hiper parâmetros do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "316e8c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[4, 5, 4, 3]\n",
      "0.03\n"
     ]
    }
   ],
   "source": [
    "print(model_mlp.getMaxIter())\n",
    "print(model_mlp.getLayers())\n",
    "print(model_mlp.getStepSize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ed76a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164810f",
   "metadata": {},
   "source": [
    "### Modificando os Hiper Parâmetros do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "380efba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parunico = {model_mlp.maxIter: 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "216d2970",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 10:22:58 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:22:59 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:23:00 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:23:00 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "23/03/05 10:23:00 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "23/03/05 10:23:00 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625\n",
      "23/03/05 10:23:00 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.03125\n",
      "23/03/05 10:23:00 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0234375\n",
      "23/03/05 10:23:00 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "23/03/05 10:23:05 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:23:05 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "23/03/05 10:23:05 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "23/03/05 10:23:06 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:23:06 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:23:06 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:23:06 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "23/03/05 10:23:06 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "23/03/05 10:23:06 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625\n",
      "23/03/05 10:23:06 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.046875\n",
      "23/03/05 10:23:06 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "23/03/05 10:23:08 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:23:08 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "23/03/05 10:23:08 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "23/03/05 10:23:08 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "23/03/05 10:23:08 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "23/03/05 10:23:08 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.09375\n",
      "23/03/05 10:23:08 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n"
     ]
    }
   ],
   "source": [
    "model_mlp = obj_mlp.fit(iris_train, parunico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79ab0cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[4, 5, 4, 3]\n",
      "0.03\n"
     ]
    }
   ],
   "source": [
    "print(model_mlp.getMaxIter())\n",
    "print(model_mlp.getLayers())\n",
    "print(model_mlp.getStepSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49d248d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|dependente|prediction|\n",
      "+----------+----------+\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "|       0.0|       0.0|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previsao_test = model_mlp.transform(iris_test)\n",
    "previsao_test.select(\"dependente\", \"prediction\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a569c4d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "acuracia = avaliar.evaluate(previsao_test)\n",
    "\n",
    "print(f\"Acurácia: {acuracia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f885c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
